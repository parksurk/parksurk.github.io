---
title: "Beyond current NLP with HMTL(ë²ˆì—­)"
excerpt: "HMTLë¡œ NLPì˜ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ë›°ì–´ ë„˜ë‹¤."
date: 2018-11-23 18:00:00 -0400
categories:
  - NLP
tags:
  - NLP
  - DL
---

* * *

HMTLë¡œ NLPì˜ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ë›°ì–´ ë„˜ë‹¤.
============================
(Orinial Blog = https://medium.com/huggingface/beating-the-state-of-the-art-in-nlp-with-hmtl-b4e1d5c3faf?fbclid=IwAR32nqSOE2K3HxYZ0pKYSOxgdcE8dD52H8ZZ_9YIzaJQJWX1H6N9FI3EVLc)

HMTL = **H**ierarchical **M**ulti-**T**ask **L**earning model

ìµœê·¼ NLPë¶„ì•¼ê°€ ë– ì˜¤ë¥´ë©´ì„œ **ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ (Multi-Task Learning** )ì´ë¼ê³  ë¶ˆë¦¬ëŠ” ë”¥ëŸ¬ë‹ (Deep-Learning) ë° ì¸ê³µ ì§€ëŠ¥ (Artificial Intelligence) ê¸°ìˆ ì´ ìƒˆë¡­ê²Œ ì†Œê°œë˜ê³  ìˆìŠµë‹ˆë‹¤.

ì €(Victor Sanh)ëŠ” ê±°ì˜ 1ë…„ ë™ì•ˆ Multi-Task Learningì„ ì‹¤í—˜í•´ ì™”ê³  ê·¸ ê²°ê³¼ê°€ HMTLì…ë‹ˆë‹¤. HMTLì€ ì—¬ëŸ¬ ê°€ì§€ NLP ì‘ì—…ì—ì„œ ìµœì²¨ë‹¨ ê¸°ìˆ ì„ ëŠ¥ê°€í•˜ëŠ” ëª¨ë¸ì´ë©° ê¹Œë‹¤ë¡œìš´ ë…¼ë¬¸ì‹¬ì‚¬ë¡œ ìœ ëª…í•œ AAAI(Association for the Advancement of Artificial Intelligence) 2019 êµ­ì œí•™íšŒì—ì„œ ë°œí‘œë  ì˜ˆì •ì…ë‹ˆë‹¤. ìµœê·¼ ë¦´ë¦¬ì¦ˆëœ [ë…¼ë¬¸][] ê³¼ [ì½”ë“œ][] ë„ ì°¸ì¡° ë°”ëë‹ˆë‹¤.

ì ê·¸ëŸ¬ë©´..., Multi-Task Learningì´ë€ ë¬´ì—‡ì¼ê¹Œìš”?

> ë‹¤ì¤‘ íƒœìŠ¤í¬ í•™ìŠµì€ ë‹¨ì¼ ì•„í‚¤í…ì²˜ê°€ ë™ì‹œì— ì—¬ëŸ¬ ê°€ì§€ íƒœìŠ¤í¬ë¥¼ í•™ìŠµí•˜ë„ë¡ í›ˆë ¨ë˜ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì˜ˆì…ë‹ˆë‹¤. HMTLì„ ëŒ€í™”ì‹ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ë©‹ì§„ [ì˜¨ë¼ì¸ ë°ëª¨][] ë¥¼ ë§Œë“¤ì—ˆìœ¼ë‹ˆ ì§ì ‘ ì‹œë„í•´ë³´ì„¸ì˜¤! ğŸ®

![](https://cdn-images-1.medium.com/freeze/max/60/1*gktD4knaQIf2JSn_DyiBJg.png?q=20)

![](https://cdn-images-1.medium.com/max/2000/1*gktD4knaQIf2JSn_DyiBJg.png)

![](https://cdn-images-1.medium.com/max/2000/1*gktD4knaQIf2JSn_DyiBJg.png)

HMTL [ì˜¨ë¼ì¸ ë°ëª¨][] ì—ì„œ ì–»ì€ ê²°ê³¼ì˜ ì˜ˆ

ì „í†µì ìœ¼ë¡œ íŠ¹ì • ëª¨ë¸ì€ ì´ëŸ¬í•œ ê° NLP ì‘ì—… (Named-Entity Recognition, Entity Mention Detection, Relation Extraction, Coreference Resolution)ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

HMTLì˜ ê²½ìš°, ëª¨ë“  ê²°ê³¼ê°€ **ë‹¨ì¼ ëª¨ë¸(Single Model)** ê³¼ **Single Forward Path** ë¡œ ë¶€í„° ë‚˜ì˜µë‹ˆë‹¤!

ê·¸ëŸ¬ë‚˜ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì€ ì—¬ëŸ¬ ëª¨ë¸ ëŒ€ì‹  ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ì„ ì¤„ì´ëŠ” ë‹¨ìˆœí•œ ë°©ë²• ì´ìƒì˜ ê²ƒì…ë‹ˆë‹¤.

MTL (Multi-Task Learning)ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ë‹¤ë¥¸ ì‘ì—…ê°„ì—ë„ ê³µìœ  í•  ìˆ˜ìˆëŠ” Embeddingì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Multi-Task Learningì˜ ê·¼ë³¸ì ì¸ Motivation ì¤‘ í•˜ë‚˜ëŠ” richer representations ì„ ìœ ë„í•˜ì—¬ ê° Taskë“¤ë¡œ ë¬´í„° ë‚˜ì˜¤ëŠ” **ì´ì (Benefit)** ì„ ëª¨ë‘ ê°€ì ¸ì˜¤ê² ë‹¤ëŠ” ìƒê°ì…ë‹ˆë‹¤.

ì´ ê¸€ì—ì„œëŠ” HMTLì´ NLP Applicationsì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ê°•ë ¥í•˜ê³  ë‹¤ìš©ë„ì˜ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì´ ê°€ëŠ¥í•œì§€ì— ëŒ€í•´ ì•Œë ¤ ë“œë¦¬ë ¤ê³ í•©ë‹ˆë‹¤. ë¨¼ì €, ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì´ ì™œ í¥ë¯¸ë¡œìš´ Trendì¸ì§€ì— ëŒ€í•œ ëª‡ ê°€ì§€ ì§ê´€ì„ ê³µìœ í•©ë‹ˆë‹¤.

### Multi-Task Learningì— ëŒ€í•œ ê°„ëµí•œ ì†Œê°œì™€ ì¤‘ìš”í•œ ì´ìœ .

ê³ ì „ì ì¸ ê¸°ê³„ í•™ìŠµ Setupì‹œì— Single Taskë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´ Single Loss Functionì„ ìµœì í™”í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. ê´€ì‹¬ìˆëŠ” (í•˜ë‚˜ì˜) ì‘ì—…ì— ì´ˆì ì„ ë§ì¶”ëŠ” ê²ƒì´ ê¸°ê³„ í•™ìŠµì˜ ë§ì€ ë¬¸ì œì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì ‘ê·¼ ë°©ë²•ì´ì§€ë§Œ ë‹¤ë¥¸ ê´€ë ¨ (ë˜ëŠ” ëŠìŠ¨í•˜ê²Œ ê´€ë ¨ëœ) ì‘ì—…ì´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ê°€ì ¸ì˜¬ ìˆ˜ìˆëŠ” ì •ë³´ëŠ” í™œìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

![](https://cdn-images-1.medium.com/max/1600/1*o9jhC2FfdCzFj_mm8vXebA.jpeg)

\- _ì¶œì²˜ : Cheezburger.com_

ì„¸ê³„ ê¸°ë¡ (2018.11 í˜„ì¬) íƒ€ì´í‹€ ë³´ìœ ì ì¸ ìš°ì‚¬ì¸ ë³¼íŠ¸ (Usain Bolt, 1919 ì¶œìƒ, ì˜¬ë¦¼í”½ ê¸ˆë©”ë‹¬ë¦¬ìŠ¤íŠ¸ ğŸ¥‡) ì‚¬ë¡€ë¥¼ ì‚´í´ë´…ì‹œë‹¤. ì‹¤ì œë¡œ ìš°ì‚¬ì¸ì˜ ê²½ìš° í›ˆë ¨ì‹œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì€ ë‹¬ë¦¬ê¸°(Running)ì´ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ìš´ë™ë“¤ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, weights, box jumps, bounds ë“± ì…ë‹ˆë‹¤.ì´ ìš´ë™ë“¤ì€ ë‹¬ë¦¬ê¸°ì™€ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì—†ì§€ë§Œ ê¶ê·¹ì ì¸ ëª©í‘œ ì¸ ë‹¨ê±°ë¦¬ë‹¬ë¦¬ê¸°(Sprint)ì—ì„œ ìì‹ ì˜ ê·¼ë ¥ê³¼ í­ë°œë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

> "ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì€ ê´€ë ¨ ì‘ì—…ì˜ í•™ìŠµ Signalì— í¬í•¨ëœ ë„ë©”ì¸ ì •ë³´ë¥¼ Inductive Biasìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ Generalizationë¥¼ í–¥ìƒì‹œí‚¤ëŠ” Inductive Transferí•˜ê¸° ìœ„í•œ í•˜ë‚˜ì˜ ì ‘ê·¼ ë°©ë²•ì…ë‹ˆë‹¤. Shared Representationì„ ì‚¬ìš©í•˜ë©´ì„œ ë³‘ë ¬ë¡œ ì‘ì—…ì„ í•™ìŠµí•˜ì—¬ ì´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê° ê³¼ì œì— ëŒ€í•´ ë°°ìš´ ê²ƒì´ ë‹¤ë¥¸ ê³¼ì œë¥¼ ë” ì˜ í•™ìŠµí•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "R. Caruana [ì°¸ì¡°1](#ref1)

Natural Language Processingì—ì„œ MTLì€ R. Collobertì™€ J. Weston [ì°¸ì¡°2](#ref2) ì˜ ì‹ ê²½ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì—ì„œ ì²˜ìŒìœ¼ë¡œ í™œìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ë“¤ì´ ì œì•ˆí•œ ëª¨ë¸ì€ ì—¬ëŸ¬ ì‘ì—… (ì‘ì—… ë³„ ë ˆì´ì–´ í¬í•¨) ì´ ì„œë¡œ ë‹¤ë¥¸ ì‘ì—…ì˜ Supervised-Leaningì— ì˜í•´ í›ˆë ¨ ëœ ë™ì¼í•œ **ê³µìœ  ì„ë² ë”©(Shared Embedding)** ì— ì˜ì¡´í•˜ëŠ” MTL ì¸ìŠ¤í„´ìŠ¤ì…ë‹ˆë‹¤.

ì„œë¡œ ë‹¤ë¥¸ ì‘ì—…ê°„ì— ë™ì¼í•œ Representationì„ ê³µìœ í•˜ëŠ” ê²ƒì€ **í•œ ê°€ì§€ ì‘ì—…ì—ì„œ ë‹¤ë¥¸ ì‘ì—… ìœ¼ë¡œ ê´€ë ¨ ì§€ì‹ì„ ì „ë‹¬** í•˜ëŠ” ì•„ì£¼ ë‚®ì€ ìˆ˜ì¤€ì˜ ì‹ í˜¸ / ë°©ë²•ì²˜ëŸ¼ ë“¤ë¦´ ìˆ˜ ìˆì§€ë§Œ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ìˆëŠ” ëŠ¥ë ¥ì— íŠ¹íˆ ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì´ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.

ì •ë³´ê°€ íƒœìŠ¤í¬ê°„ì— ì „ì†¡ë˜ëŠ” ë°©ë²•ì„ ë¯¸ë¦¬ ê³ ì¹˜ëŠ” ê²ƒì€ ì¼ë°˜ì ì´ë©° ì§ì ‘ì ìœ¼ë¡œ ì´ë£¨ì–´ ì§€ì§€ë§Œ ì£¼ì–´ì§„ íƒœìŠ¤í¬ì— ê°€ì¥ ì í•©í•œ ê³„ì¸µê³¼ í•¨ê»˜ ê³µìœ  í•  ë§¤ê°œ ë³€ìˆ˜ì™€ ê³„ì¸µì„ ëª¨ë¸ ìì²´ì—ì„œ ê²°ì •í•˜ê²Œ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤ Ruder et al., 2017 [ì°¸ì¡°3](#ref3).

ìµœê·¼ì—, ì´ëŸ¬í•œ ê³µìœ ëœ í‘œí˜„(Shared Represetation)ì— ëŒ€í•œ ì•„ì´ë””ì–´ëŠ” ì˜ì—­ ì „ë°˜ì— ê±¸ì³ ì‚¬ìš©ë  ìˆ˜ ìˆê³  íŠ¹ì • íƒœìŠ¤í¬ ë§Œì˜ ê²ƒì´ ì•„ë‹Œ "**Universal Sentence Embeddings**"ë¥¼ í†µí•´ ë‹¤ì‹œ ì£¼ëª© ë°›ê³  ìˆìŠµë‹ˆë‹¤ (Conneau ë“± [ì°¸ì¡°4](#ref4) ). ëª‡ ê°€ì§€ ì‹œë„ë“¤ì´ MTLì— ì˜ì¡´í•˜ê³  ìˆëŠ”ë°... ì˜ˆë¥¼ ë“¤ì–´, Subramanian et al. [ì°¸ì¡°5](#ref5) ì€ ë‹¤ì–‘í•œ ì‘ì—…ì— ê±¸ì³ ì¼ë°˜í™” í•  ìˆ˜ìˆìŒì„  ì—°êµ¬í•˜ì˜€ëŠ”ë°, ì´ ì¼ë°˜í™”ëŠ” ë¬¸ì¥ì˜ ë‹¤ì¤‘ ì–¸ì–´ ì–‘ìƒ(multiple linguistic aspect)ì„ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤ê³  ì£¼ì¥í•˜ê³  **Gensen** ì„ ì œì•ˆí–ˆëŠ”ë° ì´ëŠ” ê³µìœ ëœ ì¸ì½”ë” í‘œí˜„(Shared Encoder Represetation)ê³¼ ëª‡ ê°€ì§€ ì‘ì—…ë³„ ë ˆì´ì–´ê°€ ë’¤ë”°ë¥´ëŠ” MTL ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œ ì‚¬ìš© ëœ 6 ê°€ì§€ ì‘ì—…ì€ ì„œë¡œ ê´€ë ¨ì´ ëŠìŠ¨í•œ í…ŒìŠ¤í¬ì¸  'Natural Language Inference' ì—ì„œ ë¶€í„° 'Machine Translation through Constituency Parsing' ê¹Œì§€ì˜ ë‹¤ì–‘í•œ ë²”ìœ„ì— ì´ë¦…ë‹ˆë‹¤.

ì´ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ë‹¤ìŒ ë¸”ë¡œê·¸ë¥¼ ì°¸ê³ í•˜ì‹­ì‹œìš”. [blog post on Universal Word and Sentence EmbeddingsğŸ“š][]

ìš”ì»¨ëŒ€, ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì€ ë§ì€ ê´€ì‹¬ì„ ëŒê³  ìˆìœ¼ë©° NLPì˜ ë‹¤ì–‘í•œ ë¬¸ì œì— ëŒ€í•´ ì ìš©ì‹œ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë‚´ìš©ì…ë‹ˆë‹¤. ì´ëŠ” ë˜í•œ Computer Vision ğŸ‘€ ì˜ì—­ì— ëŒ€í•´ì—ì„œë„ ë§ˆì°¬ì…ë‹ˆë‹¤. [**GLUE ë²¤ì¹˜ ë§ˆí¬**][]  (General Language Understanding Evaluation, Wang et al. [ì°¸ì¡°6](#ref6) ) ì™€ ê°™ì€ ë²¤ì¹˜ ë§ˆí¬ ëŠ” ìµœê·¼ì— MTL ì•„í‚¤í…ì²˜ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ Language Understanding ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.

NLPì—ì„œ MTLì— ëŒ€í•œë³´ë‹¤ í¬ê´„ì  ì¸ ê°œìš”ë¥¼ ë³´ë ¤ë©´  S. Ruderì˜ [_ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸_][] ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.ğŸ“š

### íŒŒì´ì¬ì—ì„œ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ ğŸ

ğŸ”¥ ì´ì œ MTLì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ë³´ì´ëŠ”ì§€ ëª‡ ê°€ì§€ ì½”ë“œë¥¼ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

Multi-Task Learning Schemeì˜ ë§¤ìš° ì¤‘ìš”í•œ ë¶€ë¶„ì€ **íŠ¸ë ˆì´ë„ˆ(Trainer)** ì…ë‹ˆë‹¤. : ë„¤íŠ¸ì›Œí¬ë¥¼ ì–´ë–»ê²Œ Trainí•´ì•¼í•©ë‹ˆê¹Œ? ì—¬ëŸ¬ ê°€ì§€ í…ŒìŠ¤í¬ë¥¼ ì–´ëŠ ìˆœì„œë¡œ ìˆ˜í–‰í•´ì•¼ í•˜ë‚˜ìš”? ì£¼ê¸°ì ìœ¼ë¡œ í…ŒìŠ¤í¬ë¥¼ ìŠ¤ìœ„ì¹­í•´ì•¼ í•˜ë‚˜ìš”? ê°™ì€ ìˆ˜ì˜ Epochë¥¼ ì¨ì•¼ í•˜ë‚˜ìš”? í˜„ì¬ê¹Œì§€ëŠ” ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ í•©ì˜ê°€ ì—†ìœ¼ë©°, ë§ì€ ì„œë¡œë‹¤ë¥¸ Train ì ˆì°¨ê°€ ë‹¤ì–‘í•œ ë¬¸í—Œì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

ë¨¼ì €, ìš°ë¦¬ê°€ ì„ íƒí•˜ëŠ” Train ê³¼ì •ì— ë§ëŠ” ê°„ë‹¨í•˜ê³  ì¼ë°˜ì ì¸ ì½”ë“œë¡œ ì‹œì‘í•´ ë³´ê² ìŠµë‹ˆë‹¤.

*   **Select a task** (ì„ íƒí•œ ì•Œê³ ë¦¬ì¦˜ì´ ë¬´ì—‡ì´ë“  ìƒê´€ì—†ìŠµë‹ˆë‹¤).
*   **Select a batch**(ì¼ë°˜ì ìœ¼ë¡œ Randomí•˜ê²Œ ë°°ì¹˜ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì´ ì•ˆì „í•œ ì„ íƒì…ë‹ˆë‹¤).
*   **Perform a forward pass**
*   **Propagate the loss** (backward pass = Back Propagation)

ì´ 4 ë‹¨ê³„ëŠ” ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— ì í•©í•´ì•¼í•©ë‹ˆë‹¤.

**forward pass** ì¤‘ì— ëª¨ë¸ì€ í•´ë‹¹ í…ŒìŠ¤í¬ì˜ lossì„ ê³„ì‚°í•©ë‹ˆë‹¤. **backward pass** ë™ì•ˆ lossë¡œë¶€í„° ê³„ì‚°ëœ GradientëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ì „íŒŒë˜ì–´ í…ŒìŠ¤í¬ íŠ¹ì • layerê³¼ ê³µìœ ëœ ì„ë² ë”©(ë° ê¸°íƒ€ ëª¨ë“  ê´€ë ¨ëœ í•™ìŠµ ê°€ëŠ¥ ë§¤ê°œ ë³€ìˆ˜)ì„ ìµœì í™”í•©ë‹ˆë‹¤.

* * *

[Hugging Face][] ì±—ë´‡ ì‚¬ì´íŠ¸ì—ì„œ, 'Allen Institute for AI'ì—ì„œ ê°œë°œí•œ **AllenNLPì—** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. **AllenNLPì—** ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” NLPì—ì„œ ë°ì´í„°ë¡œë“œ ë° ì²˜ë¦¬ë¥¼ ìœ„í•´ PyTorchì˜ ìœ ì—°ì„±ê³¼ ìŠ¤ë§ˆíŠ¸í•œ ëª¨ë“ˆì„ ì œê³µí•˜ì—¬ NLPì˜ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•  ë•Œ ê°•ë ¥í•˜ê³  ë‹¤ì–‘í•œ ë„êµ¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì— ëŒ€í•´ì„œ ìì„¸í•œ ë‚´ìš©ì„ ì›í•˜ì‹œë©´ [AllenNLP ì˜¨ë³´ë“œ ììŠµì„œ][] ë¥¼ ì°¸ê³ í•˜ì‹­ì‹œìš”. ğŸ˜¬
* * *

ì´ì œ AllenNLPì— ê¸°ë°˜í•œ MTL íŠ¸ë ˆì´ë„ˆë¥¼ ì‘ì„±í•˜ê¸°ìœ„í•œ ê°„ë‹¨í•œ ì½”ë“œì„ ë³´ì—¬ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ë¨¼ì € í…ŒìŠ¤í¬ ê³ ìœ ì˜ ë°ì´í„° ì„¸íŠ¸ì™€ í…ŒìŠ¤í¬ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ëª¨ë“  ì†ì„±ì„ í¬í•¨ í•  í´ë˜ìŠ¤ `Task`ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

~~~python
from allennlp.data.iterators import DataIterator

class Task():
    """
    A class to encapsulate the necessary informations (and datasets)
    about each task.
    Parameters
    ----------
    name : ``str``, required
        The name of the task.
    validation_metric_name : ``str``, required
        The name of the validation metric to use to monitor training
        to select the best epoch and to stop the training based on exit condition.
    validation_metric_decreases : ``bool``, required
        Whether or not the validation metric should decrease for improvement.
    evaluate_on_test : ``bool`, optional (default = False)
        Whether or not the task should be evaluated on the test set at the end of the training.
    """
    def __init__(self,
                name: str,
                validation_metric_name: str,
                validation_metric_decreases: bool,
                evaluate_on_test: bool = False) -> None:
        self._name = name

        self._train_data = None
        self._validation_data = None
        self._test_data = None
        self._evaluate_on_test = evaluate_on_test

        self._val_metric = validation_metric_name
        self._val_metric_decreases = validation_metric_decreases

        self._data_iterator = None

    def load_data(self,
                dataset_path: str,
                dataset_type: str):
        """
        Load a dataset from a file and store it.
        Parameters
        ----------
        dataset_path: ``str``, required
            The path to the dataset.
        dataset_type: ``str``, required
            The type of the dataset (train, validation, test)
        """
        assert dataset_type in ["train", "validation", "test"]

        dataset = read(dataset_path) # Replace with whatever loading you want.
        setattr(self, "_%s_data" % dataset_type, dataset)

    def set_data_iterator(self,
                         data_iterator: DataIterator):
        self._data_iterator = data_iterator
~~~

ì´ì œ í´ë˜ìŠ¤ `Task`ê°€Â ìˆìœ¼ë¯€ë¡œ ëª¨ë¸ì„ ì •ì˜ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

AllenNLPì—ì„œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì€ ë§¤ìš° ì‰½ìŠµë‹ˆë‹¤.  [allennlp.models.model.Model][] í´ë˜ìŠ¤ì—ì„œ ìƒì† ë°›ë„ë¡í•˜ì‹­ì‹œì˜¤. Train ë‹¨ê³„ì—ì„œ penalties (e.g. L1 or L2 regularizations) ë¥¼ ì ìš©í•˜ëŠ” `get_regularization_penalty()`ì™€ ê°™ì€ ìœ ìš©í•œ Methodê°€ ë§ì´ ì œê³µ ë©ë‹ˆë‹¤.

ì´ì œ ìš°ë¦¬ê°€ í•„ìš”ë¡œí•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. `forward()`ë° `get_metrics()`. ì´ ë°©ë²•ì€ ê°ê° Train ì¤‘ í˜„ì¬ ì‘ì—…ì— ëŒ€í•œ forward pass (up to the loss computation) ë° training/evaluation metricsì„ ê³„ì‚°í•©ë‹ˆë‹¤.

Multi-task Learningì„ ìœ„í•œ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” `task_name` ë¼ëŠ” íŠ¹ì • ì¸ìˆ˜ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì½”ë“œë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

~~~python
import torch

from allennlp.models.model import Model

class MyMTLModel(Model):
    def __init__(self):
        """
        Whatever you need to initialize your MTL model/architecture.
        """

    def forward(self,
                task_name: str,
                tensor_batch: torch.Tensor):
        """
        Defines the forward pass of the model. This function is designed
        to compute a loss function defined by the user.
        It should return

        Parameters
        ----------
        task_name: ``str``, required
            The name of the task for which to compute the forward pass.
        tensor_batch: ``torch.Tensor``, required
            An embedding representation of the input to pass through the model.

        Returns
        -------
        output_dict: ``Dict[str, torch.Tensor]``
            An output dictionary containing at least the computed loss for the task of interest.
        """
        raise NotImplementedError

    def get_metrics(self,
                    task_name: str):
        """
        Compute and update the metrics for the current task of interest.

        Parameters
        ----------
        task_name: ``str``, required
            The name of the current task of interest.
        Returns
        -------
        A dictionary of metrics.
        """
        raise NotImplementedError
~~~

MTLì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ training task ìˆœì„œë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” taskë¥¼ ì„ íƒí•  ë•Œ, ê° parameter update(forward + backward passes) í›„ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§í•˜ëŠ” ì‘ì—…ì„ ì„ íƒí•˜ë„ë¡í•˜ëŠ” ê°€ì¥ ì§ì ‘ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì€ ì•ì„œ ì–¸ê¸‰ í•œ **Gensen** ê³¼ ê°™ì€ ëª‡ ê°€ì§€ ì´ì „ ì—°êµ¬ì—ì„œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

 ì—¬ê¸°ì„œ ë” ì¢‹ì€ ì ì€...íƒœìŠ¤í¬ë¥¼ ì„ íƒí•  í™•ë¥ ì´ ì „ì²´ ë°°ì¹˜ ìˆ˜ì— ë¹„í•´ íƒœìŠ¤í¬ì— ëŒ€í•œ í•™ìŠµ ë°°ì¹˜ì˜ ë¹„ìœ¨ì— ë¹„ë¡€í•˜ëŠ” ë¶„í¬ë¥¼ ë”°ë¼ ì„ì˜ë¡œ íƒœìŠ¤í¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì´ ìƒ˜í”Œë§ ì ˆì°¨ëŠ” ë‚˜ì¤‘ì— ì•Œ ìˆ˜ ìˆë“¯ì´ ë§¤ìš° ìœ ìš©í•˜ë©° 'catastrophic forgetting'ì„ ë°©ì§€í•˜ê¸°ìœ„í•œ ë§¤ìš° ë©‹ì§„ ë°©ë²• ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì½”ë“œëŠ” ë°©ê¸ˆ ì„¤ëª…í•œ ì ˆì°¨ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. `task_list`ëŠ” `Task` ëª©ë¡ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

~~~python
from typing import List
import numpy as np
from allennlp.data.iterators import DataIterator

"""
Set the Data Iterator for each task
The data iterator is responsible for yield batches over the specified dataset.
"""
for task in task_list:
task_name = task._name
task.set_data_iterator(DataIterator())

"""
Set whatever DataIterator you like.
Create the sampling probability distribution over the tasks
"""
sampling_prob = [task._data_iterator.get_num_batches(task._train_data) for task in task_list]
sampling_prob = sampling_prob / np.sum(sampling_prob)
def choose_task(sampling_prob):

"""
Randomly choose one task to train.
"""
return np.argmax(np.random.multinomial(1, sampling_prob))
~~~

MTL íŠ¸ë ˆì´ë„ˆë¥¼ ì‚¬ìš©í•´ ë´…ì‹œë‹¤.

ë‹¤ìŒ ì½”ë“œëŠ” ì§€ê¸ˆê¹Œì§€ êµ¬ì¶• í•œ ê¸°ë³¸ ìš”ì†Œë“¤ì„ ì–´ë–»ê²Œ ì¡°í•© í•  ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.

ì´ `train()`ë°©ë²•ì€ ì‘ì—…ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ì‘ì—…ì„ ë°˜ë³µí•˜ê³  ì—…ë°ì´íŠ¸ í›„ MTL ëª¨ë¸ ì—…ë°ì´íŠ¸ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.

~~~python
from allennlp.training.optimizers import Optimizer


class MultiTaskTrainer():
    def __init__(self,
                model: Model,
                task_list: List[Task])
        self._model = model
        self._task_list = task_list

        self._optimizers = {}
        for task in self._task_list:
            self._optimizers[task._name] = Optimizer() # Set the Optimizer you like.
            # Each task can have its own optimizer and own learning rate scheduler.


    def train(self,
             n_epochs: int = 50):

        ### Instantiate the training generators ###
        self._tr_generators = {}
        for task in self._task_list:
            data_iterator = task._data_iterator
            tr_generator = data_iterator(task._train_data,
                                        num_epochs = None)
            self._tr_generators[task._name] = tr_generator

        ### Begin Training ###
        self._model.train() # Set the model to train mode.
        for i in range(n_epochs):
            for _ in range(total_nb_training_batches):
                task_idx = choose_task()
                task = self._task_list[task_idx]
                task_name = task._name

                next_batch = next(self._tr_generators[task._name]) # Sample the next batch for the current task of interest.

                optimizer = self._optimizers[task._name] # Get the task-specific optimizer for the current task of interest.			
                optimizer.zero_grad()

                output_dict = self._model.forward(task_name = task_name,
                                                  tensor_batch = batch) #Forward Pass

                loss = output_dict["loss"]
                loss.backward() # Backward Pass    
~~~

 validation metrics (cf `_val_metric`ì™€ `_val_metric_decreases` in í´ë˜ìŠ¤ `Task`)ì„ ê¸°ë°˜ìœ¼ë¡œí•˜ëŠ” Trainì— stopping conditionì„ í¬í•¨ì‹œí‚¤ëŠ” ê²ƒì€ í•­ìƒ ì¢‹ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ validation metricsê°€ `patience` number epoch ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ Trainì„ ì¤‘ë‹¨ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ëŒ€ê°œ ê°ê°ì˜ Train epochê°€ ì¢…ë£Œëœ í›„ì— ìˆ˜í–‰ë©ë‹ˆë‹¤. ì €ëŠ” ì•„ì§ ì™„ì„±í•˜ì§€ ëª»í•œ ìƒíƒœì´ì–´ì„œ ì´ì „ì˜ ì½”ë“œë¥¼ ì‰½ê²Œ ìˆ˜ì •í•˜ì—¬ ì´ëŸ¬í•œ ê°œì„  ì‚¬í•­ì„ ê³ ë ¤í•˜ê±°ë‚˜ ë³´ë‹¤ [ì™„ë²½í•œ ì½”ë“œ][] ì‚´í´ë³¼ ê³„íšì…ë‹ˆë‹¤.

* * *

ì´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì»¤ë²„ë§í•˜ì§€ ì•Šì•˜ì§€ë§Œ... MTL ëª¨ë¸ì„ Trainí•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ìˆëŠ” ë§ì€ ê¸°ìˆ ì´ ìˆìŠµë‹ˆë‹¤. ì´ì™€ ê´€ë ¨ëœ ë©ê°€ì§€ Referenceë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. :

*   **Successive regularization** MTL ëª¨ë¸ì„ Train í•  ë•Œ ë°œìƒí•˜ëŠ” ì£¼ìš” ë¬¸ì œ ì¤‘ í•˜ë‚˜ëŠ” **catastrophic forgetting** ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ê°‘ìê¸° ì´ì „ì— ë°°ìš´ ì‘ì—…ê³¼ ê´€ë ¨ëœ ì§€ì‹ì˜ ì¼ë¶€ë¥¼ ìƒˆ ì‘ì—…ì´ í•™ìŠµí•  ë•Œ ìŠì–´ ë²„ë¦¬ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ í˜„ìƒì€ ì—¬ëŸ¬ ì‘ì—…ì´ ìˆœì°¨ì ìœ¼ë¡œ í›ˆë ¨ ë  ë•Œ íŠ¹íˆ ë°˜ë³µë©ë‹ˆë‹¤. Hashimoto et al. [ì°¸ì¡°6](#ref6) ì´ Successive regularization ë¥¼ ë„ì…í–ˆì”ë‹ˆë‹¤. : lossì— ëŒ€í•´ L2 íŒ¨ë„í‹°ë¥¼ ì¶”ê°€í•˜ì—¬ parameter updateê°€ ì´ì „ epochì˜ parameterì—ì„œ ë„ˆë¬´ ë©€ì–´ì§€ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì—ì„œ MTL íŠ¸ë ˆì´ë„ˆëŠ” parameter update í›„ ì‘ì—…ì„ switchí•˜ì§€ ì•Šê³  í•´ë‹¹ ì‘ì—…ì— ëŒ€í•´ì„œ ì „ì²´ Train Datasetì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
*   **Multi-Task as Question Answering** : ìµœê·¼, McCann et al. [ì°¸ì¡°7](#ref7) ì€ Multi-Task Learningì„ ìˆ˜í–‰í•˜ê¸°ìœ„í•œ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì†Œê°œí–ˆìŠµë‹ˆë‹¤. ê° ì‘ì—…ì€ ì§ˆì˜ ì‘ë‹µ ì‘ì—…ìœ¼ë¡œ ì¬êµ¬ì„±ë˜ë©° ë‹¨ì¼ í†µì¼ ëª¨ë¸ ( [MQAN][] )  ì‘ì—…ì—ì„œ ê³ ë ¤ë˜ëŠ” 10 ê°€ì§€ ë‹¤ë¥¸ ì‘ì—…ì— ëŒ€í•´ ê³µë™ìœ¼ë¡œ í›ˆë ¨ë©ë‹ˆë‹¤. MQANì€ WikiSQLì˜ ì˜ë¯¸ ë¡ ì  íŒŒì‹± ì‘ì—…ê³¼ ê°™ì€ ëª‡ ê°€ì§€ ì‘ì—…ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤. ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ,ì´ ì—°êµ¬ëŠ” ë‹¨ì¼ ê³¼ì œ í•™ìŠµì˜ í•œê³„ì™€ ë‹¤ì¤‘ ê³¼ì œ í•™ìŠµê³¼ ì „ì´ í•™ìŠµì˜ ê´€ê³„ì— ëŒ€í•´ ë…¼ì˜í•©ë‹ˆë‹¤.

### ì˜ë¯¸ ë¡ ì  ê³¼ì œì—ì„œì˜ ìµœì²¨ë‹¨ ê¸°ìˆ  ê°œì„  : HMTL (Hierarchical Multi-Task Learning Model)

ì´ì œ **training scheme** ì— ëŒ€í•´ ì´ì•¼ê¸° í–ˆìœ¼ë¯€ë¡œ multi-task learning schemeì—ì„œ ê°€ì¥ ë§ì€ ì´ì ì„ ì–»ì„ ìˆ˜ìˆëŠ” **Model** ì„ ì–´ë–»ê²Œ ê°œë°œí•  ìˆ˜ ìˆì„ ì§€ì— ëŒ€í•´ì„œ ì´ì•¼ê¸° ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ì €ì˜ [ë…¼ë¬¸][] ì´ AAAI 2019ì—ì„œ ë°œí‘œ ë  ì˜ˆì •ì¸ë°...ì œê°€ ì œì•ˆí•œ ë°©ë²•ì€ **hierarchical way** ì…ë‹ˆë‹¤.

ë³´ë‹¤ ì •í™•í•˜ê²Œ ë§í•˜ìë©´, ìš°ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ í…ŒìŠ¤í¬ ê°„ì˜ ì–¸ì–´ ê³„ì¸µ êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì„ íƒëœ **semantic tasks** ì§‘í•© ì‚¬ì´ ì— **ê³„ì¸µ êµ¬ì¡°** ë¥¼ ë§Œë“­ë‹ˆë‹¤ (Hashimoto et al. [ì°¸ì¡°6](#ref6) )

![](https://cdn-images-1.medium.com/max/1200/1*CEYglzD7tsDhc1_fvjW57A.png)

**HMTL** (ê³„ì¸µì  ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ) ì•„í‚¤í…ì²˜. word representations (embeddings)ì€ **ì§§ì€ ì—°ê²°** ì„ í†µí•´ ì „ì²´ ì•„í‚¤í…ì²˜ì—ì„œ **ê³µìœ ** ë©ë‹ˆë‹¤.

ê·¸ëŸ¬í•œ ê³„ì¸µ êµ¬ì¡°ì˜ ì§ê´€ì€ ì¼ë¶€ ì‘ì—…ì´ ë‹¨ìˆœ í•  ìˆ˜ ìˆê³  ì…ë ¥ì— ëŒ€í•œ ì œí•œëœ ìˆ˜ì •ì´ ìš”êµ¬ë˜ëŠ” ë°˜ë©´ ë‹¤ë¥¸ ì‘ì—…ì€ ì…ë ¥ì— ëŒ€í•œ ì§€ì‹ê³¼ ë³µì¡í•œ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

ìš°ë¦¬ê°€ ê³ ë ¤í•œ semantic tasks ë“¤ì€ **Named Entity Recognition**, **Entity Mention Detection**, **Relation Extraction** ë° **Coreference Resolution** ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

ëª¨ë¸ì€ ì‹ ê²½ë§ì˜ ë‚®ì€ ìˆ˜ì¤€ì—ì„œ Supervised Learningë˜ëŠ” "ë” ê°„ë‹¨í•œ"ì‘ì—…ê³¼ ì‹ ê²½ë§ì˜ ìƒìœ„ ê³„ì¸µì—ì„œ Supervised Learningë˜ëŠ” "ë³´ë‹¤ ë³µì¡í•œ"ì‘ì—…ìœ¼ë¡œ ì™¼ìª½ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê³„ì¸µ ì ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

ì‹¤í—˜ì—ì„œ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì‘ì—…ì´ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì„ í†µí•´ ì„œë¡œ ì´ìµì„ ì–»ì„ ìˆ˜ ìˆìŒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤.

*   ì´ 4 ê°€ì§€ ì‘ì—…ì˜ ì¡°í•©ì€ 3 ê°€ì§€ ì‘ì—… (Named Entity Recognition, Relation Extraction and Entity Mention Detection) ì—ì„œ **state-of-the-art performance** ì„ ì œê³µí•©ë‹ˆë‹¤.
*   MTL frameworkëŠ” single task training frameworksì— ë¹„í•´ **Training Speed** ìƒë‹¹íˆ **ê°€ì†í™”í•©ë‹ˆë‹¤**.

ìš°ë¦¬ëŠ” ë˜í•œ **HMTLì—ì„œ í•™ìŠµë˜ì–´ ê³µìœ ë˜ëŠ” Shared Embeddings** ì— ëŒ€í•´ì„œë„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ë¶„ì„ì„ ìœ„í•´ ìš°ë¦¬ ëŠ” Conneau et al.ì´ ì†Œê°œ í•œ 10 ê°€ì§€ í”„ë¡œë¹™ ì‘ì—…ì¸ **SentEval** ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. [ì°¸ì¡°8](#ref8) . ì´ëŸ¬í•œ í”„ë¡œë¹™ ì‘ì—…ì€ sentence embeddingsì´ ë‹¤ì–‘í•œ **ì–¸ì–´ ì†ì„±(linguistic properties)** (í†µì‚¬ë¡ , í‘œë©´ë¡  ë° ì˜ë¯¸ë¡ =syntactic, surface and semantic)ì„ ì–¼ë§ˆë‚˜ ì˜ í¬ì°© í•  ìˆ˜ ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œí•©ë‹ˆë‹¤.

ìš°ë¦¬ì˜ ë¶„ì„ì— ë”°ë¥´ë©´ ë‚®ì€ ìˆ˜ì¤€ì˜ Shared Embeddingsì€ ì´ë¯¸ Rich Representationì„ ì¸ì½”ë”©í–ˆìœ¼ë©° ëª¨ë¸ì˜ ì•„ë˜ìª½ ë ˆì´ì–´ì—ì„œ ìƒìœ„ ë ˆì´ì–´ë¡œ ì´ë™í•˜ë©´ ë ˆì´ì–´ì˜ ìˆ¨ê²¨ì§„ ìƒíƒœê°€ **ë” ë³µì¡í•œ ì˜ë¯¸ ì •ë³´(more complex semantic information)** ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

* * *

ì´ê²ƒìœ¼ë¡œ ë‹¤ì¤‘ íƒœìŠ¤í¬ í•™ìŠµì— ëŒ€í•œ ê²°ë¡ ì„ ë§ˆì¹©ë‹ˆë‹¤. ì €í¬ê°€ ì œì•ˆí•˜ëŠ” ê³„ì¸µ ì  ëª¨ë¸ (HMTL)ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ë‹¤ë©´ [ë…¼ë¬¸][] ğŸ“ƒ ë° [ì½”ë“œ][] âŒ¨ï¸ ë¥¼ ì°¸ê³ ë°”ëë‹ˆë‹¤.

ì €í¬ëŠ” ê´œì°®ì€ [ì˜¨ë¼ì¸ ë°ëª¨][] ë„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì˜¨ë¼ì¸ ë°ëª¨ì—ì„œ HMTLì„ ì§ì ‘ ì‹œí—˜í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ®

### ì°¸ê³  ë¬¸í—Œ

##### ref1
[1] R. Caruana, [Multitask Learning][] , 1997

##### ref2
[2] R. Collobertì™€ J. Weston, [A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning][] , 2008

##### ref3
[3] [Sebastian Ruder][] , J. Bingel, I. Augenstein and A. SÃ¸gaard, [Learning what to share between loosely related tasks][] 2017

##### ref4
[4] Conneau, D. Kiela, H. Schwenk, L. Barraultì™€ A. Bordes, [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data][] , 2017

##### ref5
[5] S, Subramanian, A. Trischler, Y. Bengioì™€ CJ Pal , 2018 [Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning][]

##### ref6
[6] K.Hashimoto, C. Xiong, Y. Tsuruoka and R. Socher, [JA Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks][] , 2017

##### ref7
[7] B. McCann, N. S. Keskar, C. Xiong, R. Socher, [The Natural Language Decathlon: Multitask Learning as Question Answering][] 2018

##### ref8
[8] A. Conneau, D. Kiela, [SentEval][] : [An Evaluation Toolkit for Universal Sentence Representations][] 2018

[ë…¼ë¬¸]: https://arxiv.org/abs/1811.06031
[ì½”ë“œ]: https://github.com/huggingface/hmtl
[ì˜¨ë¼ì¸ ë°ëª¨]: https://huggingface.co/hmtl/
[blog post on Universal Word and Sentence EmbeddingsğŸ“š]: https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a
[**GLUE ë²¤ì¹˜ ë§ˆí¬**]: https://gluebenchmark.com/
[ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸]: http://ruder.io/multi-task-learning-nlp/
[Hugging Face]: https://huggingface.co/
[AllenNLP ì˜¨ë³´ë“œ ììŠµì„œ]: https://allennlp.org/tutorials
[allennlp.models.model.Model]: https://allenai.github.io/allennlp-docs/api/allennlp.models.model.html
[ì™„ë²½í•œ ì½”ë“œ]: https://github.com/huggingface/hmtl
[MQAN]: https://einstein.ai/research/blog/the-natural-language-decathlon
[Multitask Learning]: https://link.springer.com/article/10.1023/A:1007379606734
[A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning]: https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf
[Sebastian Ruder]: https://medium.com/@sebastianruder
[Learning what to share between loosely related tasks]: https://arxiv.org/abs/1705.08142
[Supervised Learning of Universal Sentence Representations from Natural Language Inference Data]: https://arxiv.org/abs/1705.02364
[Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning]: https://arxiv.org/abs/1804.00079
[A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks]: https://arxiv.org/abs/1611.01587
[The Natural Language Decathlon: Multitask Learning as Question Answering]: https://arxiv.org/abs/1806.08730
[SentEval]: https://arxiv.org/abs/1803.05449
[An Evaluation Toolkit for Universal Sentence Representations]: https://arxiv.org/abs/1803.05449
